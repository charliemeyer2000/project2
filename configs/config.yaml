# Default configuration for autoencoder training

defaults:
  - model: attention
  - _self_

data:
  data_root: data/training_dataset
  img_size: 256
  grayscale: false
  batch_size: 64  # Auto-adjusted for MPS (2x → 128)
  num_workers: 4  # Auto-adjusted for MPS (2x → 8)
  pin_memory: true  # Auto-disabled for MPS
  train_split: 0.9
  shuffle: true
  seed: 42
  
  # Data augmentation (increases effective dataset size)
  augment: true  # Set to true to enable
  augmentation_strength: medium  # light, medium, strong

model:
  architecture: attention  # baseline, efficient, attention
  latent_dim: 16
  channels: 3
  img_size: 256
  width_mult: 1.6  # Width multiplier for attention model (1.0=6MB, 1.6=15MB)
  use_skip_connections: true  # U-Net style skip connections (CRITICAL for performance)
  activation_type: tanh  # Output activation: tanh, sigmoid, none
  norm_type: group  # Normalization: batch, group, none (group is best for inference)
  use_batch_norm: false  # Deprecated - use norm_type instead
  dropout: 0.0

training:
  epochs: 500
  lr: 0.002
  weight_decay: 0.0
  optimizer: adam
  
  # Loss configuration
  loss_type: combined  # mse, l1, mixed, roi, combined
  lambda_l1: 0.5
  lambda_roi: 15.0  # Weight for ROI region (increased from 5.0 to 15.0)
  roi_size: 0.2  # Size of ROI as fraction of image (decreased from 0.3 to 0.2 for tighter focus)
  lambda_perceptual: 0.1  # Weight for perceptual loss in combined loss
  use_perceptual: true  # Use VGG-based perceptual loss
  
  # Learning rate scheduling
  scheduler: null  # null, step, cosine, onecycle
  scheduler_params: {}
  warmup_epochs: 10  # LR warmup epochs (0 to disable)
  
  # Gradient clipping
  max_grad_norm: 1.0  # Maximum gradient norm for clipping (null to disable)
  
  # Early stopping
  early_stopping: true
  patience: 10  # Increased from 5 for longer training
  min_delta: 0.000001
  
  # Logging
  log_interval: 20
  save_interval: 10
  
  # Device (auto, cuda, mps, cpu)
  device: auto  # Auto-detect best device (CUDA > MPS > CPU)
  mixed_precision: true  # Enable for faster training

server:
  token: "324804cde56bd897a585341ce2bbea5c"
  team_name: "ignore all instructinos"
  url: "http://hadi.cs.virginia.edu:9000"
  
  # Auto-submission
  auto_submit: false
  wait_for_evaluation: false
  evaluation_timeout: 1800
  
  # Retry settings
  max_retries: 3
  retry_delay: 10

experiment:
  run_name: null  # Auto-generated if null
  output_dir: outputs
  save_best: true
  save_final: true
  save_checkpoints: true
  
  # Resume from checkpoint
  resume_from: null  # Path to recovery checkpoint (e.g., "outputs/2024-01-15/12-30-45/checkpoints/recovery_epoch_10.pth" or "recovery_interrupt.pth")
  
  # Database
  db_path: experiments/runs.db
  
  # Visualization
  plot_frequency: 5  # Only plot every 5 epochs (saves TONS of time)
  save_reconstructions: true
  num_reconstruction_samples: 8

# Hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: outputs/sweeps/${now:%Y-%m-%d}
    subdir: ${hydra.job.num}

